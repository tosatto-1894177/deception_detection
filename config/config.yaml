# YAML PER CONFIGURAZIONE PROGETTO

# Percorsi delle cartelle
paths:
  # INPUT: Video originali scaricati da YouTube e partizionati in clip
  video_dir: "data/raw_clips"

  # INPUT: Frame estratti dalle clip con volti ritagliati
  frames_dir: "data/frames"

  # INPUT: Cartella con i csv delle feature estratte con Openface per ciascuna clip
  openface_csv_dir: "data/openface2"

  # INPUT: File Excel con le annotazioni MUMIN
  train_annotations: "data/metadata/train_dolos.xlsx"

  # OUTPUT: Dove salvare i checkpoint del modello durante training
  models_dir: "models/checkpoints"

  # OUTPUT: Log per TensorBoard e file di log testuale
  logs_dir: "logs"

  # OUTPUT: Risultati finali (metriche, confusion matrix, predizioni)
  results_dir: "results"


# -------------------- PREPROCESSING --------------------
# Configurazione per l'estrazione dei frame (ESEGUITO UNA VOLTA SOLA)
preprocessing:
  # FPS: Quanti frame estrarre per secondo di video
  fps: 5

  # DIMENSIONE: Risoluzione finale di ogni frame [larghezza, altezza]
  # ResNet richiede 224x224 (standard ImageNet)
  img_size: [224, 224]

  # MAX_FRAMES: Limite massimo frame per clip (per evitare clip troppo lunghe)
  max_frames: null # nessun limite (usa tutti i frame)

  # FACE DETECTION: Configurazione rilevamento volti
  face_detection:
    # METHOD: Algoritmo scelto per la rilevazione dei volti
    # - 'haar': Veloce, buon compromesso (già in OpenCV, nessuna install extra)
    # - 'mtcnn': Più accurato, serve: pip install facenet-pytorch
    # - 'retinaface': Migliore, serve: pip install retinaface-pytorch
    # method: "haar" - commentato dopo aver rimosso il codice per 'mtcnn' e 'retinaface'
    # I frame sono stati estratti con 'haar'

    # Margine percentuale attorno al volto rilevato
    # - 0.3 = aggiungi 30% spazio attorno alla bbox del volto
    margin: 0.3

    # Cosa fare se non viene rilevato un volto in un frame
    # - true: Salva comunque il frame intero (come fallback)
    # - false: Scarta il frame
    save_failed_frames: true

    # Dove eseguire face detection (haar è sempre CPU)
    device: "cpu"


# DATASET
# Configurazione caricamento dati per training
dataset:
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

  # SEED: Per riproducibilità
  seed: 42

  # Split type
  fold_idx: 1  # Per fold dolos (1, 2, o 3)

  # Se True viene implementato il modello multimodale
  use_openface: True

  # DATA AUGMENTATION
  augmentation:
    enabled: true  # Attiva/disattiva tutto

    # FLIP: Probabilità di flip orizzontale del volto
    # - 0.5 = 50% dei frame vengono flippati durante training
    random_horizontal_flip: 0.5

    # COLOR JITTER:
    # Simula diverse condizioni di luce, qualità video
    color_jitter:
      brightness: 0.2  # ±20% luminosità
      contrast: 0.2    # ±20% contrasto
      saturation: 0.1  # ±10% saturazione
      hue: 0.05        # ±5% tonalità


# MODEL
# Architettura del modello
model:
  name: "DcDt_model_1"

  # Indica se il modello da addestrare è video-only o multimodale
  type: multimodal

  # RESNET: Feature extractor
  resnet:
    # - 'resnet34': 512 features
    # - 'resnet50': 2048 features
    architecture: "resnet34"

    # PRETRAINED: Usa pesi pre-allenati su ImageNet
    # - true: Inizia con conoscenza già acquisita
    # - false: Inizia da zero
    pretrained: true

    # FREEZE_LAYERS:
    # - true: ResNet usato solo come feature extractor
    # - false: Fine-tuning (aggiorna anche ResNet)
    freeze_layers: true

    # UNFREEZE_FROM: Se freeze_layers=false, da quale layer fare fine-tuning
    # - 'layer4': Sblocca solo ultimo layer (più conservativo)
    # - 'layer1': Sblocca tutto (più aggressivo)
    unfreeze_from: "layer4"

  # TCN: Temporal Convolutional Network per catturare dinamiche temporali
  tcn:
    # NUM_LAYERS: Profondità della TCN -> più layers indicano che la tcn cattura dipendenze temporali più lunghe
    num_layers: 3

    # KERNEL_SIZE: Finestra temporale che ogni layer analizza, ovvero numero di frame "guardati" insieme. Più il numero è alto e più è ampio il campo recettivo
    kernel_size: 3

    # DROPOUT: Probabilità di "spegnere" neuroni (previene overfitting)
    # - 0.2 = 20% neuroni spenti casualmente durante training
    dropout: 0.2

    # HIDDEN_CHANNELS: Numero di canali (feature) per ogni layer
    # - [256, 256, 256]: 3 layer con 256 canali ciascuno
    hidden_channels: [256, 256, 256]

  # Configurazione branch OpenFace
  openface:
    input_dim: 49  # 17 AU intensity + 18 AU presence + 8 gaze + 6 pose
    #hidden_dim: 128
    output_dim: 64
    #tcn_channels: [128]

  # Configurazione Fusion Layer
  fusion:
    #hidden_dims: [256, 128]  # MLP dopo concatenazione (384 -> 256 -> 128 -> 2)
    hidde_dims: [192, 96] # ridotto perchè fusion_dim nuova = 256+64 = 320

  # ATTENTION: Attention pooling per valutare il "peso" dei frame
  attention:
    # - 'temporal': Attention sui frame
    # - 'self-attention': Self-attention completa
    type: "temporal"

    # HIDDEN_DIM: Dimensione layer interno attention
    hidden_dim: 128

    # NUM_HEADS: Per multi-head attention (se self-attention)
    # - 1: Single-head
    # - 4-8: Multi-head
    num_heads: 1

  # CLASSIFIER: MLP per predizione finale
  classifier:
    # - [64]: Un layer da 64 neuroni
     # - [128, 64]: Un layer da [128, 64] neuroni
    hidden_dims: [128, 64]

    # DROPOUT:
    dropout: 0.3

    # NUM_CLASSES: Numero di classi da predire
    # - 2: Truth (0) vs Deception (1) → BINARIA
    num_classes: 2


# -------------------- TRAINING --------------------
# Iperparametri per il training - configurazione pensata per training su COLAB pro
training:
  # BATCH_SIZE: Quanti video processare contemporaneamente
  # - 8: Buon compromesso per GPU con 8-12GB VRAM
  # - 2: Adatto alla GPU del PC personale
  batch_size: 8

  # NUM_EPOCHS
  num_epochs: 20

  # LEARNING_RATE: Quanto "velocemente" il modello impara
  # - 0.001: Standard per Adam e AdamW
  learning_rate: 0.001

  # WEIGHT_DECAY: Regolarizzazione L2 (previene overfitting)
  # - 0.01: Valore tipico per adamw
  weight_decay: 0.01

  # OPTIMIZER: Algoritmo di ottimizzazione
  # - 'adam': Il migliore per la maggior parte dei casi
  # - 'adamw': Adam con weight decay migliorato
  optimizer: "adamw"

  # SCHEDULER: Come ridurre learning rate nel tempo
  scheduler:
    # - 'step': Riduci ogni N epoch
    # - 'cosine': Riduzione smooth (coseno)
    # - 'plateau': Riduci quando validation non migliora
    # - null: Nessuno scheduler (LR fisso)
    type: "cosine"

    # STEP_SIZE: Ogni quante epoch ridurre LR (solo per 'step')
    step_size: 10

    # GAMMA: Fattore di riduzione
    # - 0.5: Dimezza LR
    gamma: 0.5

    # PATIENCE: Epoch di attesa prima di ridurre (solo per 'plateau')
    patience: 5

    # T_max: num epochs
    T_max: 20

  # Loss function
  loss:
    type: "cross_entropy"  # Standard per classificazione binaria

    # CLASS_WEIGHTS: Pesi per bilanciare classi sbilanciate
    # - null: Pesi automatici (calcola da dataset)
    # - [1.0, 1.5]: Penalizza di più errori sulla classe 1 (deception)
    class_weights: null

  # EARLY STOPPING: Ferma training se non migliora - evita overfitting e risparmia tempo
  early_stopping:
    enabled: true  # Attiva/disattiva

    # PATIENCE: Quante epoch aspettare senza miglioramento prima di fermare
    patience: 8

    # MIN_DELTA: Miglioramento minimo considerato "vero miglioramento"
    # - 0.001: Deve migliorare di almeno 0.1% per contare
    min_delta: 0.001

  # GRADIENT_CLIP: Limita grandezza gradienti (previene esplosioni)
  # - null: Disabilita
  gradient_clip: 1.0

  # DEVICE: Dove eseguire il training
  # - 'cuda': GPU
  # - 'cpu': CPU
  device: "cuda"

  # NUM_WORKERS: Thread paralleli
  # - 0: Disabilita parallelismo
  num_workers: 2

  # PIN_MEMORY: Ottimizzazione per trasferimento CPU→GPU
  # - true: Più veloce se si usa GPU
  # - false: Se si usa CPU o per problemi memoria
  pin_memory: true


# -------------------- VALIDATION --------------------
validation:
  # VALIDATE_EVERY: Ogni quante epoch fare validazione
  validate_every: 1

  # METRICS: Metriche da calcolare su validation set
  # - accuracy: % predizioni corrette
  # - precision: Delle predizioni "lie", quante erano vere lie?
  # - recall: Delle vere lie, quante ne ho predette correttamente?
  # - f1: Media armonica precision/recall
  # - auc_roc: Area sotto curva ROC - capacità di distinguere "truth" da "lie"
  # - confusion_matrix
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "auc_roc"
    - "confusion_matrix"

  # SAVE_BEST_ONLY
  # - true: Salva solo il modello con miglior validation
  # - false: Salva tutti i checkpoint
  save_best_only: true

  # SAVE_EVERY_N_EPOCHS: Salva checkpoint ogni N epoch (anche se non migliore)
  save_every_n_epochs: 5


# -------------------- TESTING --------------------
# Configurazione per test finale
testing:
  # CHECKPOINT_PATH: Quale modello usare per test
  checkpoint_path: "models/checkpoints/best_model.pth"

  # BATCH_SIZE: Batch size per test
  batch_size: 16

  # SAVE_PREDICTIONS: Salva predizioni in CSV
  # Output: clip_name, true_label, predicted_label, confidence
  save_predictions: true

  # SAVE_ATTENTION_MAPS: Salva visualizzazioni attention
  # Mostra: quali frame il modello ha guardato di più
  save_attention_maps: true

  # SAVE_MISCLASSIFIED: Salva esempi classificati male per capire dove sbaglia il modello
  save_misclassified: true


# -------------------- LOGGING --------------------
logging:
  # WANDB: Weights & Biases (servizio online per il tracking dgeli esperimenti)
  wandb:
    enabled: false  # Non ancora attivato account wandb
    project: "deception-detection"
    entity: null  # Username wandb
    name: "DcDt_model_1_run_1"

  # TENSORBOARD: Visualizzazione locale
  tensorboard:
    enabled: true  # visualizza con: tensorboard --logdir logs/tensorboard
    log_dir: "logs/tensorboard"

  # CONSOLE: Output su terminale
  print_every: 10  # Stampa loss ogni 10 batch

  # LOG FILE: File testuale con tutto l'output
  log_file: "logs/training.log"


# REPRODUCIBILITY
reproducibility:
  # SEED: per riproducibilità
  seed: 42

  # DETERMINISTIC: Forza algoritmi deterministici
  # - true: Stesso risultato ogni run ma più lento
  # - false: Più veloce ma risultati leggermente diversi ogni run
  deterministic: false

  # BENCHMARK: CuDNN benchmark per ottimizzazione GPU
  # - true: Più veloce ma non deterministico
  # - false: Deterministico
  # NOTA: Se deterministic=true, questo viene ignorato
  benchmark: true